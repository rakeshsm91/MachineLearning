{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression and SGD Homework \n",
    "***\n",
    "**Name**: $<$Rakesh Shivanand Margoor$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 9th**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "\n",
    "In this homework you'll implement stochastic gradient ascent for logistic regression and you'll apply it to the task of determining whether documents are talking about automobiles or motorcycles.\n",
    "\n",
    "<br>\n",
    "\n",
    "![autos_motorcycles](autos_motorcycles.jpg \"A car and a motorcycle\")\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "You should not use any libraries that implement any of the functionality of logistic regression for this assignment; logistic regression is implemented in Scikit-Learn, but you should do everything by hand now. You'll be able to use library implementations of logistic regression in the future.\n",
    "\n",
    "Here are the rules: \n",
    "\n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function or class API **do not** change it.\n",
    "- Do not change the location of the data or data directory.  Use only relative paths to access the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import collections\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Problem 1: Loading and Exploring the Data\n",
    "***\n",
    "\n",
    "The `Example` class will be used to store the features and labels associated with a single training or test example.  The `read_data` function will read in the text data and split it into training and test sets.  \n",
    "\n",
    " Load the data and then do the following: \n",
    "- Report the number of words in the vocabulary \n",
    "- Explain how the code is creating features (i.e. what text model is being used). \n",
    "- Go into the raw text files in the data directory and figure out which label (0/1) refers to which class of document (automobiles or motorcycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kSEED = 1735\n",
    "kBIAS = \"BIAS_CONSTANT\"\n",
    "\n",
    "np.random.seed(kSEED)\n",
    "\n",
    "class Example:\n",
    "    \"\"\"\n",
    "    Class to represent a document example\n",
    "    \"\"\"\n",
    "    def __init__(self, label, words, vocab):\n",
    "        \"\"\"\n",
    "        Create a new example\n",
    "\n",
    "        :param label: The label (0 / 1) of the example\n",
    "        :param words: The words in a list of \"word:count\" format\n",
    "        :param vocab: The vocabulary to use as features (list)\n",
    "        \"\"\"\n",
    "        self.nonzero = {}\n",
    "        self.y = label\n",
    "        self.x = np.zeros(len(vocab))\n",
    "        #print(len(vocab))\n",
    "        for word, count in [x.split(\":\") for x in words]:\n",
    "            if word in vocab:\n",
    "                assert word != kBIAS, \"Bias can't actually appear in document\"\n",
    "                self.x[vocab.index(word)] += float(count)\n",
    "                self.nonzero[vocab.index(word)] = word\n",
    "        #print(self.x)\n",
    "        #print(self.nonzero)\n",
    "        self.x[0] = 1\n",
    "\n",
    "def read_dataset(positive, negative, vocab, train_frac=0.9):\n",
    "    \"\"\"\n",
    "    Reads in a text dataset with a given vocabulary\n",
    "\n",
    "    :param positive: Positive examples\n",
    "    :param negative: Negative examples\n",
    "    :param vocab: A list of vocabulary words\n",
    "    :param test_frac: How much of the data should be reserved for test\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = [x.split(\"\\t\")[0] for x in open(vocab, 'r') if '\\t' in x]\n",
    "    #print(len(vocab))\n",
    "    #print(vocab)\n",
    "    assert vocab[0] == kBIAS, \\\n",
    "        \"First vocab word must be bias term (was %s)\" % vocab[0]\n",
    "\n",
    "    train_set = []\n",
    "    test_set = []\n",
    "    index = 0\n",
    "    for label, input in [(1, positive), (0, negative)]:\n",
    "        for line in open(input):\n",
    "            ex = Example(label, line.split(), vocab)\n",
    "            if np.random.random() <= train_frac:\n",
    "                train_set.append(ex)\n",
    "            else:\n",
    "                test_set.append(ex)\n",
    "\n",
    "    # Shuffle the data \n",
    "    np.random.shuffle(train_set)\n",
    "    np.random.shuffle(test_set)\n",
    "\n",
    "    return train_set, test_set, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_fname = \"../data/autos_motorcycles/positive\"\n",
    "neg_fname = \"../data/autos_motorcycles/negative\"\n",
    "voc_fname = \"../data/autos_motorcycles/vocab\"\n",
    "train_set, test_set, vocab = read_dataset(pos_fname, neg_fname, voc_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of words in the vocabulary: 5327\n",
    "Explain how the code is creating features (i.e. what text model is being used): Bag of words\n",
    "1 = motorcycles and 0 = automobiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Implementing SGD with Lazy Sparse Regularization\n",
    "***\n",
    "\n",
    "We've given you a class `LogReg` below which will train a logistic regression classifier to predict whether a document is talking about automobiles or motorcycles. \n",
    "\n",
    "**Part A**: In this problem you will modify the `sgd_update` function to perform **unregularized** stochastic gradient descent updates of the weights. Note that you should only update the weights for **non-zero** features, i.e. weights associated with words that appear in the current training example. The code below this cell demonstrates how to instantiate the class and train the classifier.   \n",
    "\n",
    "We've also given you unit tests in the next cell based on the simple example worked out in  the Lecture 4 in-class notebook.  At first your code will fail both of them. When your code is working you should pass tests called `test_unreg` and `test_learnrate`.  Do not move on to **Part A** until your code passes both of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg:\n",
    "    def __init__(self, train_set, test_set, lam, eta=0.1):\n",
    "        \"\"\"\n",
    "        Create a logistic regression classifier\n",
    "\n",
    "        :param train_set: A set of training examples\n",
    "        :param test_set: A set of test examples \n",
    "        :param lam: Regularization parameter\n",
    "        :param eta: The learning rate to use \n",
    "        \"\"\"\n",
    "        \n",
    "        # Store training and test sets \n",
    "        self.train_set = train_set\n",
    "        self.test_set = test_set \n",
    "        \n",
    "        # Initialize vector of weights to zero  \n",
    "        self.w = np.zeros_like(train_set[0].x)\n",
    "        \n",
    "        # Store regularization parameter and eta function \n",
    "        self.lam = lam\n",
    "        self.eta = eta\n",
    "        \n",
    "        # Create dictionary for lazy-sparse regularization\n",
    "        self.last_update = dict()\n",
    "\n",
    "        # Make sure regularization parameter is not negative \n",
    "        assert self.lam>= 0, \"Regularization parameter must be non-negative\"\n",
    "        \n",
    "        # Empty lists to store NLL and accuracy on train and test sets \n",
    "        self.train_nll = []\n",
    "        self.test_nll = []\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        self.iterationdict =collections.defaultdict(lambda:0)\n",
    "        \n",
    "    def sigmoid(self,score, threshold=20.0):\n",
    "        \"\"\"\n",
    "        Prevent overflow of exp by capping activation at 20.\n",
    "        You do not need to change this function. \n",
    "\n",
    "        :param score: A real valued number to convert into a number between 0 and 1\n",
    "        \"\"\"\n",
    "\n",
    "        # if score > threshold, cap value at score \n",
    "        if abs(score) > threshold:\n",
    "            score = threshold * np.sign(score)\n",
    "\n",
    "        return 1.0 / (1.0 + np.exp(-score)) \n",
    "\n",
    "    def compute_progress(self, examples):\n",
    "        \"\"\"\n",
    "        Given a set of examples, compute the NLL and accuracy\n",
    "        You shouldn't need to change this function. \n",
    "\n",
    "        :param examples: The dataset to score\n",
    "        :return: A tuple of (log probability, accuracy)\n",
    "        \"\"\"\n",
    "\n",
    "        NLL = 0.0\n",
    "        num_correct = 0\n",
    "        for ex in examples:\n",
    "            # compute prob prediction\n",
    "            p = self.sigmoid(self.w.dot(ex.x))\n",
    "            # update negative log likelihood\n",
    "            NLL = NLL - np.log(p) if ex.y==1 else NLL - np.log(1.0-p)\n",
    "            # update number correct \n",
    "            num_correct += 1 if np.floor(p+.5)==ex.y else 0\n",
    "\n",
    "        return NLL, float(num_correct) / float(len(examples))\n",
    "    \n",
    "    def train(self, num_epochs=1, isVerbose=False, report_step=5):\n",
    "        \"\"\"\n",
    "        Train the logistic regression classifier on the training data \n",
    "\n",
    "        :param num_epochs: number of full passes over data to perform \n",
    "        :param isVerbose: boolean indicating whether to print progress\n",
    "        :param report_step: how many iterations between recording progress\n",
    "        \"\"\"\n",
    "        iteration = 0\n",
    "        # Perform an epoch \n",
    "        for pp in range(num_epochs):\n",
    "            # shuffle the data  \n",
    "            np.random.shuffle(self.train_set)\n",
    "            # loop over each training example\n",
    "            for ex in self.train_set:\n",
    "                # perform SGD update of weights \n",
    "                self.sgd_update(ex, iteration)\n",
    "                # record progress \n",
    "                if iteration % report_step == 1:\n",
    "                    train_nll, train_acc = self.compute_progress(self.train_set)\n",
    "                    test_nll, test_acc = self.compute_progress(self.test_set)\n",
    "                    self.train_nll.append(train_nll)\n",
    "                    self.test_nll.append(test_nll)\n",
    "                    self.train_acc.append(train_acc)\n",
    "                    self.test_acc.append(test_acc)\n",
    "                    if isVerbose:\n",
    "                        print(\"Update {: 5d}  TrnNLL {: 8.3f}  TstNLL {: 8.3f}  TrnA {:.3f}  TstA {:.3f}\"\n",
    "                             .format(iteration-1, train_nll, test_nll, train_acc, test_acc))\n",
    "                iteration += 1\n",
    "    \n",
    "    def sgd_update(self, train_example, iteration):\n",
    "        \"\"\"\n",
    "        Compute a stochastic gradient update to improve the NLL \n",
    "\n",
    "        :param train_example: The example to take the gradient with respect to\n",
    "        :param iteration: The current iteration (an integer)\n",
    "        \"\"\"\n",
    "       # print(train_example.x)\n",
    "        # TODO implement LSR updates of weights \n",
    "        muiix = 0\n",
    "        for w, x in zip(self.w, train_example.x):\n",
    "            muiix += w * x\n",
    "        \n",
    "        muii = self.sigmoid(muiix) - train_example.y\n",
    "        lsrmul = (1- 2*self.eta*self.lam)\n",
    "\n",
    "\n",
    "        for ii in range(0, len(self.w)):\n",
    "            if train_example.x[ii] != 0:\n",
    "                self.w[ii] = self.w[ii] - (self.eta*(muii*train_example.x[ii]))\n",
    "                if ii !=0:\n",
    "                    self.w[ii] = self.w[ii] * (pow(lsrmul, (self.iterationdict[ii]) + 1))\n",
    "                    self.iterationdict[ii] = 0\n",
    "            else:\n",
    "                self.iterationdict[ii] += 1\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_unreg (__main__.TestLogReg) ... ok\n",
      "test_learnrate (__main__.TestLogReg) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.016s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aec710e080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run -i tests.py \"part A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit tests are located in the script `tests.py` in this directory.  Execute the following cell to call the script and run the tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_reg (__main__.TestLogReg) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%run -i tests.py \"part B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: After your unregularized updates are working, modify the `sgd_update` function again to perform regularized updates using **Lazy Sparse Regularization**. Note that you should not regularize the bias weight. See the Lecture 4 in-class notebook for a refresher on LSR. **Note**: After implementing LSR, your code should still pass the unit tests for **Part A** when `lam = 0`. \n",
    "\n",
    "We've given you a third unit test in the next cell called `test_reg` based on the simple example of LSR worked out in  the Lecture 4 in-class notebook.  Do not move on to **Problem 3** until your code passes the test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update     0  TrnNLL  688.405  TstNLL   78.190  TrnA 0.698  TstA 0.702\n",
      "Update     5  TrnNLL  905.698  TstNLL  106.518  TrnA 0.552  TstA 0.471\n",
      "Update    10  TrnNLL  818.772  TstNLL   97.424  TrnA 0.578  TstA 0.545\n",
      "Update    15  TrnNLL  966.793  TstNLL  108.249  TrnA 0.544  TstA 0.488\n",
      "Update    20  TrnNLL  760.917  TstNLL   85.421  TrnA 0.624  TstA 0.628\n",
      "Update    25  TrnNLL  551.120  TstNLL   62.694  TrnA 0.746  TstA 0.719\n",
      "Update    30  TrnNLL  552.019  TstNLL   62.319  TrnA 0.765  TstA 0.752\n",
      "Update    35  TrnNLL  526.411  TstNLL   59.644  TrnA 0.775  TstA 0.744\n",
      "Update    40  TrnNLL  460.686  TstNLL   50.111  TrnA 0.816  TstA 0.810\n",
      "Update    45  TrnNLL  459.168  TstNLL   48.443  TrnA 0.799  TstA 0.802\n",
      "Update    50  TrnNLL  415.244  TstNLL   44.684  TrnA 0.819  TstA 0.843\n",
      "Update    55  TrnNLL  408.459  TstNLL   43.888  TrnA 0.831  TstA 0.826\n",
      "Update    60  TrnNLL  396.779  TstNLL   42.845  TrnA 0.835  TstA 0.826\n",
      "Update    65  TrnNLL  384.545  TstNLL   42.522  TrnA 0.836  TstA 0.818\n",
      "Update    70  TrnNLL  380.353  TstNLL   41.979  TrnA 0.841  TstA 0.826\n",
      "Update    75  TrnNLL  368.386  TstNLL   42.335  TrnA 0.845  TstA 0.826\n",
      "Update    80  TrnNLL  360.582  TstNLL   40.521  TrnA 0.856  TstA 0.826\n",
      "Update    85  TrnNLL  358.376  TstNLL   41.866  TrnA 0.838  TstA 0.843\n",
      "Update    90  TrnNLL  339.031  TstNLL   39.567  TrnA 0.861  TstA 0.810\n",
      "Update    95  TrnNLL  359.888  TstNLL   40.828  TrnA 0.846  TstA 0.826\n",
      "Update   100  TrnNLL  387.638  TstNLL   42.150  TrnA 0.840  TstA 0.835\n",
      "Update   105  TrnNLL  354.137  TstNLL   41.578  TrnA 0.850  TstA 0.802\n",
      "Update   110  TrnNLL  351.837  TstNLL   41.368  TrnA 0.851  TstA 0.802\n",
      "Update   115  TrnNLL  323.161  TstNLL   39.355  TrnA 0.866  TstA 0.826\n",
      "Update   120  TrnNLL  310.837  TstNLL   38.469  TrnA 0.872  TstA 0.826\n",
      "Update   125  TrnNLL  309.714  TstNLL   37.977  TrnA 0.876  TstA 0.843\n",
      "Update   130  TrnNLL  303.189  TstNLL   36.145  TrnA 0.879  TstA 0.843\n",
      "Update   135  TrnNLL  300.584  TstNLL   36.985  TrnA 0.866  TstA 0.835\n",
      "Update   140  TrnNLL  301.706  TstNLL   37.767  TrnA 0.867  TstA 0.826\n",
      "Update   145  TrnNLL  291.400  TstNLL   36.682  TrnA 0.880  TstA 0.835\n",
      "Update   150  TrnNLL  293.991  TstNLL   38.769  TrnA 0.876  TstA 0.826\n",
      "Update   155  TrnNLL  299.646  TstNLL   38.003  TrnA 0.877  TstA 0.860\n",
      "Update   160  TrnNLL  271.519  TstNLL   34.616  TrnA 0.894  TstA 0.868\n",
      "Update   165  TrnNLL  278.092  TstNLL   34.346  TrnA 0.893  TstA 0.868\n",
      "Update   170  TrnNLL  285.271  TstNLL   35.620  TrnA 0.888  TstA 0.868\n",
      "Update   175  TrnNLL  264.920  TstNLL   35.371  TrnA 0.893  TstA 0.860\n",
      "Update   180  TrnNLL  262.751  TstNLL   34.812  TrnA 0.894  TstA 0.860\n",
      "Update   185  TrnNLL  259.512  TstNLL   34.011  TrnA 0.894  TstA 0.860\n",
      "Update   190  TrnNLL  263.398  TstNLL   33.825  TrnA 0.898  TstA 0.876\n",
      "Update   195  TrnNLL  270.902  TstNLL   34.166  TrnA 0.899  TstA 0.884\n",
      "Update   200  TrnNLL  277.238  TstNLL   34.429  TrnA 0.889  TstA 0.876\n",
      "Update   205  TrnNLL  290.342  TstNLL   34.987  TrnA 0.881  TstA 0.868\n",
      "Update   210  TrnNLL  257.168  TstNLL   32.467  TrnA 0.899  TstA 0.868\n",
      "Update   215  TrnNLL  268.684  TstNLL   32.620  TrnA 0.894  TstA 0.876\n",
      "Update   220  TrnNLL  248.666  TstNLL   31.303  TrnA 0.903  TstA 0.884\n",
      "Update   225  TrnNLL  236.589  TstNLL   31.237  TrnA 0.903  TstA 0.868\n",
      "Update   230  TrnNLL  227.256  TstNLL   34.867  TrnA 0.914  TstA 0.868\n",
      "Update   235  TrnNLL  227.752  TstNLL   35.003  TrnA 0.915  TstA 0.868\n",
      "Update   240  TrnNLL  216.876  TstNLL   31.252  TrnA 0.920  TstA 0.893\n",
      "Update   245  TrnNLL  215.949  TstNLL   31.416  TrnA 0.922  TstA 0.893\n",
      "Update   250  TrnNLL  215.846  TstNLL   31.366  TrnA 0.922  TstA 0.893\n",
      "Update   255  TrnNLL  213.423  TstNLL   31.262  TrnA 0.922  TstA 0.893\n",
      "Update   260  TrnNLL  212.803  TstNLL   32.071  TrnA 0.923  TstA 0.884\n",
      "Update   265  TrnNLL  204.037  TstNLL   30.027  TrnA 0.922  TstA 0.884\n",
      "Update   270  TrnNLL  204.322  TstNLL   29.503  TrnA 0.922  TstA 0.893\n",
      "Update   275  TrnNLL  202.466  TstNLL   29.814  TrnA 0.923  TstA 0.884\n",
      "Update   280  TrnNLL  201.679  TstNLL   29.707  TrnA 0.923  TstA 0.884\n",
      "Update   285  TrnNLL  230.942  TstNLL   30.609  TrnA 0.909  TstA 0.884\n",
      "Update   290  TrnNLL  201.003  TstNLL   29.239  TrnA 0.924  TstA 0.876\n",
      "Update   295  TrnNLL  200.546  TstNLL   29.399  TrnA 0.923  TstA 0.876\n",
      "Update   300  TrnNLL  200.108  TstNLL   29.337  TrnA 0.923  TstA 0.868\n",
      "Update   305  TrnNLL  210.983  TstNLL   32.379  TrnA 0.926  TstA 0.876\n",
      "Update   310  TrnNLL  208.223  TstNLL   32.167  TrnA 0.924  TstA 0.876\n",
      "Update   315  TrnNLL  205.612  TstNLL   31.801  TrnA 0.928  TstA 0.884\n",
      "Update   320  TrnNLL  195.558  TstNLL   28.809  TrnA 0.933  TstA 0.901\n",
      "Update   325  TrnNLL  194.174  TstNLL   29.824  TrnA 0.933  TstA 0.893\n",
      "Update   330  TrnNLL  194.252  TstNLL   30.606  TrnA 0.929  TstA 0.876\n",
      "Update   335  TrnNLL  195.748  TstNLL   31.727  TrnA 0.930  TstA 0.876\n",
      "Update   340  TrnNLL  182.995  TstNLL   31.280  TrnA 0.937  TstA 0.876\n",
      "Update   345  TrnNLL  215.981  TstNLL   37.173  TrnA 0.917  TstA 0.876\n",
      "Update   350  TrnNLL  204.866  TstNLL   36.163  TrnA 0.922  TstA 0.876\n",
      "Update   355  TrnNLL  185.162  TstNLL   32.638  TrnA 0.934  TstA 0.893\n",
      "Update   360  TrnNLL  180.269  TstNLL   30.295  TrnA 0.935  TstA 0.893\n",
      "Update   365  TrnNLL  174.047  TstNLL   29.184  TrnA 0.936  TstA 0.917\n",
      "Update   370  TrnNLL  175.268  TstNLL   28.164  TrnA 0.935  TstA 0.909\n",
      "Update   375  TrnNLL  170.006  TstNLL   27.961  TrnA 0.939  TstA 0.917\n",
      "Update   380  TrnNLL  169.599  TstNLL   28.654  TrnA 0.938  TstA 0.909\n",
      "Update   385  TrnNLL  169.140  TstNLL   28.497  TrnA 0.939  TstA 0.909\n",
      "Update   390  TrnNLL  167.994  TstNLL   27.917  TrnA 0.942  TstA 0.917\n",
      "Update   395  TrnNLL  166.070  TstNLL   27.568  TrnA 0.941  TstA 0.917\n",
      "Update   400  TrnNLL  165.629  TstNLL   27.515  TrnA 0.942  TstA 0.917\n",
      "Update   405  TrnNLL  165.562  TstNLL   27.516  TrnA 0.942  TstA 0.917\n",
      "Update   410  TrnNLL  165.686  TstNLL   27.519  TrnA 0.943  TstA 0.917\n",
      "Update   415  TrnNLL  173.656  TstNLL   27.708  TrnA 0.932  TstA 0.909\n",
      "Update   420  TrnNLL  174.639  TstNLL   27.775  TrnA 0.932  TstA 0.909\n",
      "Update   425  TrnNLL  194.684  TstNLL   29.759  TrnA 0.927  TstA 0.909\n",
      "Update   430  TrnNLL  180.287  TstNLL   27.755  TrnA 0.933  TstA 0.917\n",
      "Update   435  TrnNLL  183.488  TstNLL   28.093  TrnA 0.935  TstA 0.917\n",
      "Update   440  TrnNLL  182.197  TstNLL   28.071  TrnA 0.936  TstA 0.926\n",
      "Update   445  TrnNLL  175.847  TstNLL   27.932  TrnA 0.935  TstA 0.909\n",
      "Update   450  TrnNLL  169.222  TstNLL   27.405  TrnA 0.937  TstA 0.901\n",
      "Update   455  TrnNLL  150.195  TstNLL   27.320  TrnA 0.953  TstA 0.917\n",
      "Update   460  TrnNLL  148.701  TstNLL   27.767  TrnA 0.953  TstA 0.934\n",
      "Update   465  TrnNLL  148.640  TstNLL   28.195  TrnA 0.951  TstA 0.926\n",
      "Update   470  TrnNLL  145.183  TstNLL   27.930  TrnA 0.955  TstA 0.926\n",
      "Update   475  TrnNLL  136.514  TstNLL   27.848  TrnA 0.954  TstA 0.909\n",
      "Update   480  TrnNLL  136.203  TstNLL   27.828  TrnA 0.957  TstA 0.909\n",
      "Update   485  TrnNLL  136.155  TstNLL   27.749  TrnA 0.957  TstA 0.909\n",
      "Update   490  TrnNLL  135.587  TstNLL   27.272  TrnA 0.956  TstA 0.909\n",
      "Update   495  TrnNLL  136.497  TstNLL   27.737  TrnA 0.956  TstA 0.917\n",
      "Update   500  TrnNLL  133.250  TstNLL   27.190  TrnA 0.957  TstA 0.917\n",
      "Update   505  TrnNLL  133.335  TstNLL   27.218  TrnA 0.957  TstA 0.917\n",
      "Update   510  TrnNLL  132.589  TstNLL   26.855  TrnA 0.957  TstA 0.917\n",
      "Update   515  TrnNLL  134.045  TstNLL   27.027  TrnA 0.955  TstA 0.909\n",
      "Update   520  TrnNLL  136.010  TstNLL   28.315  TrnA 0.955  TstA 0.917\n",
      "Update   525  TrnNLL  132.331  TstNLL   27.570  TrnA 0.957  TstA 0.926\n",
      "Update   530  TrnNLL  133.754  TstNLL   27.667  TrnA 0.957  TstA 0.917\n",
      "Update   535  TrnNLL  135.657  TstNLL   28.397  TrnA 0.954  TstA 0.917\n",
      "Update   540  TrnNLL  134.847  TstNLL   28.417  TrnA 0.956  TstA 0.917\n",
      "Update   545  TrnNLL  140.905  TstNLL   30.801  TrnA 0.949  TstA 0.909\n",
      "Update   550  TrnNLL  137.270  TstNLL   29.963  TrnA 0.952  TstA 0.917\n",
      "Update   555  TrnNLL  136.662  TstNLL   29.847  TrnA 0.952  TstA 0.917\n",
      "Update   560  TrnNLL  136.325  TstNLL   29.828  TrnA 0.952  TstA 0.917\n",
      "Update   565  TrnNLL  119.029  TstNLL   27.342  TrnA 0.962  TstA 0.926\n",
      "Update   570  TrnNLL  118.165  TstNLL   27.415  TrnA 0.961  TstA 0.926\n",
      "Update   575  TrnNLL  118.034  TstNLL   27.350  TrnA 0.961  TstA 0.926\n",
      "Update   580  TrnNLL  128.029  TstNLL   27.954  TrnA 0.958  TstA 0.917\n",
      "Update   585  TrnNLL  124.466  TstNLL   28.060  TrnA 0.958  TstA 0.909\n",
      "Update   590  TrnNLL  121.280  TstNLL   28.031  TrnA 0.961  TstA 0.909\n",
      "Update   595  TrnNLL  121.940  TstNLL   28.712  TrnA 0.957  TstA 0.917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update   600  TrnNLL  121.927  TstNLL   29.034  TrnA 0.958  TstA 0.926\n",
      "Update   605  TrnNLL  122.448  TstNLL   29.135  TrnA 0.958  TstA 0.926\n",
      "Update   610  TrnNLL  124.221  TstNLL   29.631  TrnA 0.954  TstA 0.917\n",
      "Update   615  TrnNLL  139.443  TstNLL   28.596  TrnA 0.956  TstA 0.917\n",
      "Update   620  TrnNLL  191.461  TstNLL   32.745  TrnA 0.943  TstA 0.893\n",
      "Update   625  TrnNLL  185.229  TstNLL   31.909  TrnA 0.947  TstA 0.909\n",
      "Update   630  TrnNLL  182.750  TstNLL   30.806  TrnA 0.948  TstA 0.909\n",
      "Update   635  TrnNLL  183.816  TstNLL   31.411  TrnA 0.948  TstA 0.909\n",
      "Update   640  TrnNLL  186.747  TstNLL   33.244  TrnA 0.947  TstA 0.901\n",
      "Update   645  TrnNLL  186.673  TstNLL   33.228  TrnA 0.947  TstA 0.901\n",
      "Update   650  TrnNLL  175.775  TstNLL   30.309  TrnA 0.951  TstA 0.909\n",
      "Update   655  TrnNLL  187.617  TstNLL   32.078  TrnA 0.947  TstA 0.893\n",
      "Update   660  TrnNLL  174.548  TstNLL   29.788  TrnA 0.953  TstA 0.909\n",
      "Update   665  TrnNLL  183.312  TstNLL   30.882  TrnA 0.951  TstA 0.909\n",
      "Update   670  TrnNLL  192.019  TstNLL   32.872  TrnA 0.941  TstA 0.893\n",
      "Update   675  TrnNLL  164.625  TstNLL   27.333  TrnA 0.956  TstA 0.917\n",
      "Update   680  TrnNLL  163.635  TstNLL   26.945  TrnA 0.956  TstA 0.917\n",
      "Update   685  TrnNLL  156.140  TstNLL   24.259  TrnA 0.965  TstA 0.917\n",
      "Update   690  TrnNLL  164.412  TstNLL   27.282  TrnA 0.954  TstA 0.917\n",
      "Update   695  TrnNLL  165.361  TstNLL   27.563  TrnA 0.953  TstA 0.917\n",
      "Update   700  TrnNLL  163.473  TstNLL   26.997  TrnA 0.955  TstA 0.917\n",
      "Update   705  TrnNLL  163.096  TstNLL   26.908  TrnA 0.955  TstA 0.917\n",
      "Update   710  TrnNLL  153.230  TstNLL   24.944  TrnA 0.961  TstA 0.909\n",
      "Update   715  TrnNLL  153.041  TstNLL   24.986  TrnA 0.961  TstA 0.909\n",
      "Update   720  TrnNLL  151.583  TstNLL   24.888  TrnA 0.964  TstA 0.917\n",
      "Update   725  TrnNLL  150.820  TstNLL   24.828  TrnA 0.960  TstA 0.917\n",
      "Update   730  TrnNLL  149.838  TstNLL   24.562  TrnA 0.964  TstA 0.917\n",
      "Update   735  TrnNLL  150.147  TstNLL   23.550  TrnA 0.966  TstA 0.917\n",
      "Update   740  TrnNLL  151.302  TstNLL   23.256  TrnA 0.964  TstA 0.909\n",
      "Update   745  TrnNLL  148.532  TstNLL   22.361  TrnA 0.966  TstA 0.917\n",
      "Update   750  TrnNLL  149.267  TstNLL   22.375  TrnA 0.965  TstA 0.917\n",
      "Update   755  TrnNLL  149.856  TstNLL   22.295  TrnA 0.964  TstA 0.917\n",
      "Update   760  TrnNLL  144.768  TstNLL   21.410  TrnA 0.967  TstA 0.917\n",
      "Update   765  TrnNLL  140.454  TstNLL   21.010  TrnA 0.969  TstA 0.909\n",
      "Update   770  TrnNLL  145.347  TstNLL   21.848  TrnA 0.967  TstA 0.917\n",
      "Update   775  TrnNLL  148.383  TstNLL   22.683  TrnA 0.963  TstA 0.926\n",
      "Update   780  TrnNLL  149.056  TstNLL   22.540  TrnA 0.964  TstA 0.934\n",
      "Update   785  TrnNLL  137.639  TstNLL   20.274  TrnA 0.969  TstA 0.926\n",
      "Update   790  TrnNLL  131.322  TstNLL   20.105  TrnA 0.970  TstA 0.926\n",
      "Update   795  TrnNLL  130.872  TstNLL   19.997  TrnA 0.970  TstA 0.934\n",
      "Update   800  TrnNLL  130.955  TstNLL   19.655  TrnA 0.968  TstA 0.934\n",
      "Update   805  TrnNLL  130.179  TstNLL   19.462  TrnA 0.968  TstA 0.926\n",
      "Update   810  TrnNLL  130.107  TstNLL   19.453  TrnA 0.968  TstA 0.926\n",
      "Update   815  TrnNLL  133.604  TstNLL   21.061  TrnA 0.969  TstA 0.934\n",
      "Update   820  TrnNLL  132.053  TstNLL   20.746  TrnA 0.970  TstA 0.934\n",
      "Update   825  TrnNLL  132.963  TstNLL   21.741  TrnA 0.972  TstA 0.926\n",
      "Update   830  TrnNLL  132.901  TstNLL   21.738  TrnA 0.972  TstA 0.926\n",
      "Update   835  TrnNLL  138.944  TstNLL   20.849  TrnA 0.965  TstA 0.934\n",
      "Update   840  TrnNLL  141.413  TstNLL   21.274  TrnA 0.965  TstA 0.934\n",
      "Update   845  TrnNLL  129.032  TstNLL   19.277  TrnA 0.970  TstA 0.926\n",
      "Update   850  TrnNLL  117.492  TstNLL   18.375  TrnA 0.975  TstA 0.942\n",
      "Update   855  TrnNLL  117.523  TstNLL   18.383  TrnA 0.975  TstA 0.942\n",
      "Update   860  TrnNLL  116.544  TstNLL   18.337  TrnA 0.977  TstA 0.942\n",
      "Update   865  TrnNLL  116.610  TstNLL   18.320  TrnA 0.976  TstA 0.942\n",
      "Update   870  TrnNLL  114.466  TstNLL   18.477  TrnA 0.980  TstA 0.934\n",
      "Update   875  TrnNLL  114.185  TstNLL   18.357  TrnA 0.980  TstA 0.934\n",
      "Update   880  TrnNLL  108.583  TstNLL   18.715  TrnA 0.980  TstA 0.934\n",
      "Update   885  TrnNLL  107.676  TstNLL   18.115  TrnA 0.979  TstA 0.934\n",
      "Update   890  TrnNLL  104.413  TstNLL   18.195  TrnA 0.982  TstA 0.934\n",
      "Update   895  TrnNLL  104.018  TstNLL   18.118  TrnA 0.983  TstA 0.934\n",
      "Update   900  TrnNLL  103.893  TstNLL   18.087  TrnA 0.983  TstA 0.934\n",
      "Update   905  TrnNLL  103.186  TstNLL   18.042  TrnA 0.982  TstA 0.934\n",
      "Update   910  TrnNLL  102.278  TstNLL   17.715  TrnA 0.982  TstA 0.934\n",
      "Update   915  TrnNLL  101.775  TstNLL   17.497  TrnA 0.981  TstA 0.934\n",
      "Update   920  TrnNLL  101.666  TstNLL   17.480  TrnA 0.981  TstA 0.934\n",
      "Update   925  TrnNLL  100.320  TstNLL   17.357  TrnA 0.982  TstA 0.934\n",
      "Update   930  TrnNLL   99.535  TstNLL   17.147  TrnA 0.982  TstA 0.934\n",
      "Update   935  TrnNLL   99.580  TstNLL   16.985  TrnA 0.982  TstA 0.934\n",
      "Update   940  TrnNLL   99.440  TstNLL   16.920  TrnA 0.982  TstA 0.934\n",
      "Update   945  TrnNLL  100.014  TstNLL   16.803  TrnA 0.979  TstA 0.934\n",
      "Update   950  TrnNLL   99.809  TstNLL   16.774  TrnA 0.979  TstA 0.934\n",
      "Update   955  TrnNLL  100.271  TstNLL   16.821  TrnA 0.979  TstA 0.934\n",
      "Update   960  TrnNLL   99.779  TstNLL   16.715  TrnA 0.979  TstA 0.934\n",
      "Update   965  TrnNLL   97.785  TstNLL   16.913  TrnA 0.981  TstA 0.950\n",
      "Update   970  TrnNLL   98.420  TstNLL   16.958  TrnA 0.981  TstA 0.942\n",
      "Update   975  TrnNLL   94.925  TstNLL   16.828  TrnA 0.983  TstA 0.950\n",
      "Update   980  TrnNLL   93.775  TstNLL   16.533  TrnA 0.982  TstA 0.950\n",
      "Update   985  TrnNLL   92.613  TstNLL   15.971  TrnA 0.983  TstA 0.950\n",
      "Update   990  TrnNLL   92.074  TstNLL   15.923  TrnA 0.983  TstA 0.950\n",
      "Update   995  TrnNLL   91.527  TstNLL   15.811  TrnA 0.983  TstA 0.942\n",
      "Update  1000  TrnNLL   90.455  TstNLL   15.778  TrnA 0.988  TstA 0.942\n",
      "Update  1005  TrnNLL   90.481  TstNLL   15.794  TrnA 0.987  TstA 0.942\n",
      "Update  1010  TrnNLL   95.835  TstNLL   15.786  TrnA 0.986  TstA 0.950\n",
      "Update  1015  TrnNLL   91.301  TstNLL   15.006  TrnA 0.988  TstA 0.950\n",
      "Update  1020  TrnNLL   84.847  TstNLL   13.862  TrnA 0.990  TstA 0.950\n",
      "Update  1025  TrnNLL   84.346  TstNLL   13.434  TrnA 0.989  TstA 0.950\n",
      "Update  1030  TrnNLL   83.836  TstNLL   13.297  TrnA 0.988  TstA 0.959\n",
      "Update  1035  TrnNLL   83.836  TstNLL   13.268  TrnA 0.988  TstA 0.967\n",
      "Update  1040  TrnNLL   84.320  TstNLL   13.978  TrnA 0.990  TstA 0.950\n",
      "Update  1045  TrnNLL   84.221  TstNLL   13.938  TrnA 0.990  TstA 0.950\n",
      "Update  1050  TrnNLL   83.865  TstNLL   13.175  TrnA 0.989  TstA 0.967\n",
      "Update  1055  TrnNLL   82.490  TstNLL   13.519  TrnA 0.990  TstA 0.950\n",
      "Update  1060  TrnNLL   82.491  TstNLL   13.493  TrnA 0.990  TstA 0.950\n",
      "Update  1065  TrnNLL   82.255  TstNLL   13.548  TrnA 0.990  TstA 0.950\n"
     ]
    }
   ],
   "source": [
    "lr = LogReg(train_set, test_set, lam=0, eta=0.1)\n",
    "lr.train(isVerbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 3: Hyperparameter Tuning \n",
    "***\n",
    "\n",
    "**Part A**: Perform a systematic study of the effect of the regularization parameter on the accuracy of your classifier on the test set.  Which choice of `lam` seems to do the best?  Justify your conclusion with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9421, 0.9339, 0.9587, 0.9752, 0.9339, 0.9091, 0.9174, 0.8926, 0.8678, 0.8843, 0.7851, 0.7107, 0.7355, 0.5041, 0.5702, 0.5785, 0.7438]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xd8VGXa//HPlV5IAqQASeiEDhII\noKALCu4CImosC1ZsKFhXd33cXZ9d18ffuuvqqmtH14YVBREbNlARQXqXXpNQQggJpJf798c5wSGE\nZEgyc2aS6/16zYszM2fO+WYS5pr7Pve5jxhjUEoppU4lwOkASimlfJsWCqWUUrXSQqGUUqpWWiiU\nUkrVSguFUkqpWmmhUEopVSstFMqviYgRkW5urtvJXj/I07mUakq0UCiPEJFdIjLa6Ry+SCw7RGSj\n01mUcocWCqW871dAAtBFRAZ7c8famlL1oYVCeZWItBKRT0QkW0Ry7eVkl+e/FZGHReRHETkmIh+L\nSKyIvCUi+SKyTEQ6VdvsOPsb+iER+ZeIBNjbChSRx+zHdwAXVMtyvYj8LCJH7dffcorMoSJyRET6\nujwWLyJFIpIgInH2z3FERA6LyMKqDKdwHfAR8Jm97Lqv1iLyqohk2e/PHJfnLhKR1fb7sF1ExtiP\nn9B6E5EHReRNe7mqu+1GEdkDzLcff19E9otInoh8LyJ9XF4fLiKPi8hu+/kf7Mc+FZE7quVdKyIX\n1/KzqiZAC4XytgDgVaAj0AEoAp6pts5E4BogCegKLLZf0xr4GfhrtfUvAdKAgcBFwA324zcD44FU\n+/nLqr3uoP18NHA98ISIDKwe2BhTAswGJrk8fAXwnTHmIHAvkAHEA22APwE1zo0jIhF2jrfs20QR\nCXFZZQYQAfTBanU8Yb9uCPAG8AegJVarZFdN+ziFEUAv4Df2/c+BFHsfK+0sVR4DBgHDsN7z+4BK\n4HXgapef5Qys39Fnp5FD+SNjjN701ug3rA+x0W6sNwDIdbn/LfBnl/uPA5+73L8QWO1y3wBjXO5P\nA76xl+cDt7o892t7/aBTZJkD3HWK50YDO1zuLwKutZcfwmohdHPj570ayAaCgFDgCHCJ/Vw7rA/k\nVjW87kXgCXfea+BB4E17uZP9M3epJVNLe50YrEJeBJxRw3qhwGEgxb7/GPCc039revP8TVsUyqtE\nJEJEXrS7NfKB74GWIhLostoBl+WiGu63qLbZvS7Lu4FEezmxhudcs4wVkSV2d9ERYBwQd4ro84Fw\nERkqIh2xCtyH9nP/ArYBX9pdWPefYhtgdTXNNMaUm19aKlXdT+2Bw8aY3Bpe1x7YXst263L8fbC7\n5P5hd1/l80vLJM6+hdW0LzvvTOBqu2ttElYLSDVxWiiUt90L9ACGGmOisbpQAKQB22zvstwByLKX\n99XwnLUzkVBgFta34jbGmJZYXSg15jDGVGJ9SE4CrgQ+McYctZ87aoy51xjTBavFc4+IjKq+DftY\nzHlYH7T7RWQ/VjfUOBGJw/owby0iLWuIsBerG64mBVjdVVXa1vQjuCxfidVFNxqrFdGpKiJwCCiu\nZV+vA1cBo4BCY8ziU6ynmhAtFMqTgkUkzOUWBERhtQqOiEhrTj7eUB9/sA+StwfuAt6zH58J3Cki\nySLSCnD9ph+C1ZWSDZSLyFisrqnavA38FuuD8u2qB0VkvIh0ExEB8oEK+1bdNcAWrEI5wL51xzq+\nMckYsw/r2MFz9s8TLCJVhfS/wPUiMkpEAkQkSUR62s+txjrWESwiNR2LqS4KKAFysArM36uesAvi\nK8C/RSTRbn2cZRdW7MJQidUlqK2JZkILhfKkz7CKQtXtQeBJIBzrm+sSYF4j7OcjYAXWB+anWB+q\nAC8BXwBrsA7Yzq56gd0auBOrmORifcueW9tOjDE/YX17T8T6QK+SAnwNHMM68P6cMebbGjZxnf3c\nftcb8AK/dD9dA5QBm7AOtt9t73sp9gF3IA/4DmtAAMD/YrUAcoG/4VLETuENrG64TGAj1u/B1e+B\ndcAyrGMS/+TEz4o3gH7Am3XsRzURYoxeuEgp5T4RuRaYYow52+ksyju0RaGUcps9vHcaMN3pLMp7\nPFYoROQVETkoIutP8byIyH9EZJt90s5J49eVUr5DRH6DdUznAHV3b6kmxJMtiteAMbU8PxarbzcF\nmAI878EsSqkGMsZ8YYyJNMZcZIwpdzqP8h6PFQpjzPdYB8JO5SLgDWNZgjWWvp2n8iillKofJycI\nS+LEk6Ey7Mf2VV9RRKZgtTqIjIwc1LNnz+qrKKWUqsWKFSsOGWPi6/NaJwtFTSc21TgEyxgzHfvg\nWVpamlm+fLkncymlVJMjIrvrXqtmTo56yuDEs2aT+eWMWqWUUj7CyUIxF7jWHv10JpBnn5mqlFLK\nh3is60lE3gFGAnEikoE1VUMwgDHmBayzdsdhTaZWiHXWqVJKKR/jsUJhjJlUx/MGuM1T+1dKKdU4\n9MxspZRStdJCoZRSqlZaKJRSStVKC4VSSqla+V2hyC8qczqCUko1K35XKHYfLqS8otLpGEop1Wz4\nXaEAqNRrLSmllNf4aaHQSqGUUt6ihUIppVSt/LJQVGjfk1JKeY1fFopKPZatlFJe45+FQruelFLK\na/yyUFRooVBKKa/xy0KhLQqllPIe/ywUeoxCKaW8xi8LhXY9KaWU9/hloajU4bFKKeU1/lkotEWh\nlFJe46eFwukESinVfPhlodAzs5VSynv8slBo15NSSnmPFgqllFK18stCoV1PSinlPX5ZKIpKK5yO\noJRSzYZfFoq9uYVOR1BKqWbDLwvFrkNaKJRSylv8rlAEBwaw57AWCqWU8ha/KxQhgQHszilwOoZS\nSjUb/lcogrRFoZRS3hTkdIDTFRIUwKFjpRwrKafSGCKCAwkK9Lt6p5RSfsPvPmFDg6zI6zLyGPmv\nb5nwzCK2Zx9zOJVSSjVdflcoQuzWw98+3kBuYSlZeUVc+PQPvL98L0bP2FZKqUbnf4XCblFs2n+U\nC/q1Y95dv6J/cgx/+GAtd7+3mqPFZQ4nVEqppsXvCkVggNAyIpgAgbtHd6dtTBhv3XQm957fnY/X\nZDH+6R9Ys/eI0zGVUqrJ8LtCATCqZxsmD+tMt4QWgFU87hiVwsxbzqK8wnDp8z8y/fvteiU8pZRq\nBOLJfn0RGQM8BQQCLxtj/lHt+Y7AK0A8cBi42hiTUds209LSzPLly0/5fF5hGf8zay3zNuxnRPd4\nHrv8DOKjQhv6oyillF8TkRXGmLT6vNZjLQoRCQSeBcYCvYFJItK72mqPAW8YY/oDDwGPNHS/MRHB\nPH/1QB6+uC+Ld+Qw9qmFLNya3dDNKqVUs+XJrqchwDZjzA5jTCnwLnBRtXV6A9/YywtqeL5eRISr\nz+zI3NuH0yoimGtfWco/522irKKyMTavlFLNiicLRRKw1+V+hv2YqzXApfbyJUCUiMRW35CITBGR\n5SKyPDvb/dZBz7bRzL39bCYO7sDz327n8hcWs1fP6lZKqdPiyUIhNTxW/YDI74ERIrIKGAFkAuUn\nvciY6caYNGNMWnx8/GmFCA8J5JH0fjx75UC2Zx9j3FML+XhN1mltQymlmjNPFooMoL3L/WTghE9o\nY0yWMSbdGJMK/Nl+LM8TYS7o347P7jyHbm1acMc7q7h/1loKSk6qSUopparxZKFYBqSISGcRCQEm\nAnNdVxCROBGpyvBHrBFQHtO+dQQzbzmLaSO78t7yvYx6/Ds+Wp2pZ3QrpVQtPFYojDHlwO3AF8DP\nwExjzAYReUhEJtirjQQ2i8gWoA3w/zyVp0pwYAD3jenJB7eeRXxUKHe9u5orXlzM+kyPNGSUUsrv\nefQ8Ck+o6zyK01FRaXh/+V7+9cVmDheWMnFwB37/6+7EttDzLpRSTUtDzqPwu2nGG1NggDBxSAfG\n9mvHU19v5fXFu/h0bRb3nN+dq8/s6HfTlx/ML2ZDVj7rM/PILSxj2rldidOip5RqoGbdoqhu64Gj\n/O3jjfyw7RDd27Tgrxf2YXi3OI/sqyGMMWTkFrEhK+94YViflU/20ZLj6wQGCP2SYnh3ypmEBQc6\nmFYp5Qsa0qLQQlGNMYYvNx7g4U83svdwEWP6tOXPF/SifesIj+2zNpWVhp05BazPtIrChqw81mfm\nk1dkzZIbGCCkJLSgT2IMfZOi6ZsUQ6920fywNZupb61kXN92PD0plYCAmkYrK6WaCy0UHlBcVsHL\nC3fw7ILtVBrDLSO6MnVEV8JDPPftvKyikm0Hj51QFDZm5VNQWgFY1+Lo2S7ql6KQGEOPtlGnbDG8\n+N12Hvl8E9NGduW+MT09llsp5fu0UHjQvrwiHvlsE3PXZJEYE8afLujFBf3aIdKwb+jFZRVs3n+U\n9XYLYWNWHj/vP0ppuTXNSERIIL3bWS2EPonWv90SWhB8GsdNjDH86cN1vLN0L49e1p8r0trX/SKl\nVJOkhcILlu48zF/nbuDnffkM7dyaByf0oVe7aLdee6yknJ/32ccSMq2WwtaDx6iwp0GPDguib1LM\nCUWhU2wkgY3QXVRWUckNry1j8fYc3rhhCMN88JiLUsrztFB4SUWl4d1le3jsi83kFZVx1dCO3HN+\nd1pFhhxf50hh6QkHmDdk5rEzp4CqtzmuRejxbqO+SdH0SYwhuVV4g1sotckvLuPS537kQH4xs6cN\nP34dD6VU86GFwsuOFJby5NdbmbFkN1FhQVw2MJm9uYWsz8wn80jR8fWSWoYfbyFUFYeE6DBHMu89\nXMglzy0iPCSQOdOG67kiSjUzWigcsml/Pn+bu5HFO3LoHBf5S1FItLqQXFsavmDVnlwmTl9C36QY\n3rppqA6bVaoZ0ULhsNLySkKC/OPkvM/W7WPaWyuZcEYiT00c4NEuL6WU7/DJK9w1J/5SJADG9WvH\nfWN6MHdNFk98tcXpOEopP9Csp/BorqaO6MruQ4X8Z/42OsZGcumgZKcjKaV8mBaKZkhEePiSvuzN\nLeT+2WtJahXOmV1OurCgUkoB2vXUbAUHBvD8VYPo0DqCW2asYEf2MacjKaV8lBaKZiwmIphXJw8h\nKEC44bVlHC4odTqSUsoHaaFo5jrERjD92jSy8oq5ZcZySsornI6klPIxWigUgzq24vHLz2DZrlzu\nn7VOLw2rlDqBHsxWAFx4RiK7cwp47MstdIyN4O7R3Z2OpJTyEVoo1HG3nduNnYcKefLrrXSKjeTi\n1CSnIymlfIB2PanjRIRH0vsxtHNr7vtgLUt3HnY6klLKB2ihUCcICQrgxWsGkdwqnFtmLGfXoQKn\nIymlHKaFQp2kZUQIr0weDMANry3jSKEOm1WqOdNCoWrUKS6S6demkZFbxC0zVhy/8p5SqvnRQqFO\naXCn1jx6WX9+2nmY+2ev1WGzSjVTOupJ1eri1CR25RTw5Ndb6RwbyR2jUpyOpJTyMi0Uqk53jUph\nd04hj3+1hQ6xEVw0QIfNKtWcaNeTqpOI8I9L+zGkU2v+8MFaVuzWYbNKNSdaKJRbQoMCefGaQSTG\nhHHzGyvYk1PodCSllJdooVBuaxVpDZutNIbrX1tKXmGZ05GUUl6ghUKdli7xLXjx6kHsOVzI1Ld0\n2KxSzYEWCnXahnaJ5R/p/flxew4PzNHZZpVq6nTUk6qXSwclszungP/M30anuEimjezmdCSllIdo\noVD19rvzu7Mrp5BH522mY+tILujfzulISikP0K4nVW8iwqOX9SetYyvumbmalXtynY6klPIALRSq\nQcKCrWGzbaLDmPLGcvYe1mGzSjU1Hi0UIjJGRDaLyDYRub+G5zuIyAIRWSUia0VknCfzKM+IbRHK\nK5MHU1peyQ2vLSOvSIfNKtWUeKxQiEgg8CwwFugNTBKR3tVWewCYaYxJBSYCz3kqj/KsbgkteOGa\nQew8VMBtb62krEKHzSrVVHiyRTEE2GaM2WGMKQXeBS6qto4Bou3lGCDLg3mUhw3rGscj6f34Ydsh\n/vLReh02q1QT4clRT0nAXpf7GcDQaus8CHwpIncAkcDomjYkIlOAKQAdOnRo9KCq8Vye1p5dOQU8\nu2A7nWIjuWVEV6cjKaUaqM4WhYjcLiKt6rFtqeGx6l8xJwGvGWOSgXHADBE5KZMxZroxJs0YkxYf\nH1+PKMqb7j2/Bxf0b8c/5m1i3vp9TsdRSjWQO11PbYFlIjLTPjhdUwGoSQbQ3uV+Mid3Ld0IzAQw\nxiwGwoA4N7evfFRAgPD45WcwoH1L7n5vNWv2HnE6klKqAeosFMaYB4AU4L/AZGCriPxdROrqU1gG\npIhIZxEJwTpYPbfaOnuAUQAi0gurUGSf1k+gfFJYcCAvXZtGXItQbnx9ORm5OmxWKX/l1sFsYx2V\n3G/fyoFWwAci8mgtrykHbge+AH7GGt20QUQeEpEJ9mr3AjeLyBrgHWCy0SOgTUZci1BenTyYkvIK\nbnxtOfnFOmxWKX8kdX0ui8idwHXAIeBlYI4xpsw+lrDVGOPVo5VpaWlm+fLl3tylaqAfth5i8qtL\nGdYtjleuSyMoUM/zVMrbRGSFMSatPq91539sHJBujPmNMeZ9Y0wZgDGmEhhfn52q5uXslDgevrgv\n32/J5ra3V1JcVuF0JKXUaXCnUHwGHL/2pYhEichQAGPMz54KppqWiUM68Jfxvfly4wEmvbSEnGMl\nTkdSSrnJnULxPHDM5X6B/ZhSp+WGszvz3JUD2ZiVz6XP/8iuQwVOR1JKucGdQiGuB5jtLiednlzV\ny9h+7Xj75qHkFZWR/vyPOuOsUn7AnUKxQ0TuFJFg+3YXsMPTwVTTNahja2ZPG05UWBCTpi9h3vr9\nTkdSStXCnUJxKzAMyOSXaTimeDKUavo6x0Uya+owerWLZupbK3ht0U6nIymlTqHOLiRjzEGsk+WU\nalRxLUJ55+YzufPdVTz48UYycov407heBAS4e/K/Usob6iwUIhKGNdVGH6wzpwEwxtzgwVyqmQgP\nCeSFqwfx0McbePmHnWTlFfHvKwYQFhzodDSllM2drqcZWPM9/Qb4DmvOpqOeDKWal8AA4cEJffjz\nuF58tm4/V7/8E7kFpU7HUkrZ3CkU3Ywx/wsUGGNeBy4A+nk2lmpuRISbf9WFZ65MZW1GHpe+8CN7\ncnR+KKV8gTuFomqCniMi0hfrAkOdPJZINWvj+yfy5k1DyTlWSvrzi3TmWaV8gDuFYrp9PYoHsGZ/\n3Qj806OpVLM2pHNrZk0dRlhwIBOnL+HrjQecjqRUs1ZrobAn/ss3xuQaY743xnQxxiQYY170Uj7V\nTHVLaMGH04aT0qYFU2YsZ8aS3U5HUqrZqrVQ2Gdh3+6lLEqdID4qlHennMm5PRL43znreeTzn6ms\n1FnolfI2d7qevhKR34tIexFpXXXzeDKlgIiQIF68ZhBXDe3Ai9/t4K73VlNSrrPPKuVN7szZVHW+\nxG0ujxmgS+PHUepkQYEBPHxxX5JbRfDPeZs4kF/MS9ekERMR7HQ0pZoFdy6F2rmGmxYJ5VUiwtSR\nXXlq4gBW7znCpS/8yN7DOnxWKW9w58zsa2t63BjzRuPHUap2Fw1Iok10GFPeWE768z/yynWD6Zcc\n43QspXxa5pGiBr3enWMUg11u5wAPAhNqe4FSnnRml1hmTR1GSGAAv52+mAWbDjodSSmfVV5RyVUv\nLWnQNtzperrD5XYzkAqENGivSjVQSpsoPpw2jM5xkdz0xnLeWbrH6UhK+aTZKzPZ1cBZDupzlftC\nIKVBe1WqESREhzHzlrM4JyWOP85ex2NfbMblGltKNXul5ZU89c1W+jewe7bOQiEiH4vIXPv2CbAZ\n+KhBe1WqkUSGBvHytWlMHNyeZxZs456Zaygtr3Q6Vr0Vl1Xw3ZZs1mfmOR1FNQHvLd9L5pEi7jm/\ne4O2487w2MdclsuB3caYjAbtValGFBQYwCPp/UhuFc5jX25hf14xL1wziJhw/xg+m3WkiAWbD7Jg\n00EWbcuhqKyC2MgQlvxpFMGB9Wn0K2V96Xh2/jbSOrZiRPf4Bm3LnUKxB9hnjCkGEJFwEelkjNnV\noD0r1YhEhNvPSyGxZTj3fbCWy1/4kdeuH0Jiy3Cno52kvKKSVXuPMH+TVRw27bdm7U9uFc7lacm0\njgzhya+38v2WbEb1auNwWuWv3v5pD/vzi/n3b89ApGEXA3OnULyPdSnUKhX2Y4MbtGelPCB9YDJt\nosO4dcYKLnluEa9MHkyfROeHzx4uKOX7LdnM33SQ77Zkk1dURlCAkNapFX8c25PzeibQLaEFIkJZ\nRSUzFu9m9spMLRSqXgpLy3nu220M6xrLsK5xDd6eO4UiyBhz/CoyxphSEdFRT8pnDe8Wx/tTz+L6\nV5dxxQuLee7qQQ1uep8uYwwb9+WzYNNB5m86yOq9R6g0ENcihNG92nBezwTO6R5HdNjJ3WPBgQFc\neEYiby/dQ15hmZ6Brk7bG4t3c+hYKS9e07BjE1XcKRTZIjLBGDMXQEQuAg41yt6V8pCebaP5cNpw\nJr+6lBteW8Yjl/TjisHtPbrPgpJyfth2iAWbDrJg80EO5JcA0D85hjvOS+G8ngn0S4px65rglw5M\n5rUfd/Hpun1cObSDR3OrpuVocRkvfLedEd3jGdSxcablc6dQ3Aq8JSLP2PczgBrP1lbKl7SNCeP9\nW89i2lsruW/WWjKPFHH36JQG99e62nmo4Hhh+GnHYUorKokKDeKc7nGM7JHAyB7xJESF1b2havom\nRZOS0IIPV2VooVCn5dVFuzhSWMa9v26c1gS4USiMMduBM0WkBSDGGL1etvIbUWHBvDJ5MH+avY6n\nvtlK5pEiHknvV+/RRKXllSzdedg6EL35IDsPFQDQNT6S64Z15NyeCaR1bE1IUMNGK4kI6QOT+ee8\nTezOKaBjbGSDtqeah7zCMl5auIPze7ehf3LLRtuuO3M9/R141BhzxL7fCrjXGPNAo6VQyoOCAwN4\n9LL+JLUK58mvt3Igv5jnrhpIVA3HB2pyIL/4eKvhh62HKCitICQogLO6xDJ5WCfO7ZFAh9iIRs99\ncWoij36xiQ9XZXL36Mb7dqiarpcW7uBocXmDz5uozp2up7HGmD9V3THG5IrIOKxLoyrlF0SEu0d3\nJ6llOH+cvY7LX1jMa9cPoW3Myd1CFZWGNRlHjh+I3pCVD0C7mDAuSk3ivB4JDOsWS0SIO/996q9d\nTDjDu8Yxe2Umd41q3C4z1fTkHCvh1UU7uaB/O3q1i27Ubbvzlx4oIqHGmBKwzqMAQhs1hVJecnla\ne9pEhzHtrZVc8twiXr1+MD3bRpNXWMZ3W7NZYA9fPVxQSoDAoI6tuG9MD87rmUCPNlFe/7C+JDWJ\ne99fw4rduaR10uuFqVN78fsdFJVV8LvRjT/DkjuF4k3gGxF51b5/PfB6oydRykt+1T2embecxfWv\nLeXy5xfTq100K/bkUlFpaBURzIju8ZzbM4ER3eNpGeHsSPAxfdvywJz1zFqZqYVCndLBo8W8sXgX\nFw9IoltCVKNv352D2Y+KyFpgNCDAPKBjoydRyot6J1rDZ+9+bzXHisuZOqIr5/ZMYED7lgS6MXzV\nWyJDgxjbty2frM3irxf2Jiw40OlIygc9t2A7ZRWGO0d5Zr5WdztZ9wOVwBXATmCWR9Io5UWJLcOZ\nectZTseoU/rAZGavymT+poOM69fO6TjKx2QdKeLtn/Zw2cBkOsV5ZnTcKcfwiUh3EfmLiPwMPAPs\nxRoee64x5plTva7aNsaIyGYR2SYi99fw/BMistq+bRGRI/X+SZRqos7qGkvb6DBmr9S5ONXJnlmw\nDYPhjlHdPLaP2loUm4CFwIXGmG0AIvI7dzcsIoHAs8D5WCfpLRORucaYjVXrGGN+57L+HVgXRVJK\nuQgMEC5OTeLlhTs4dKyEuBY6lkRZ9uQUMnPZXiYN6UByq8Yfol2ltrOCLsXqclogIi+JyCisYxTu\nGgJsM8bssOeKehe4qJb1JwHvnMb2lWo20gcmUV5p+HhNltNRlA/5z/ytBAYIt5/nudYE1FIojDEf\nGmN+C/QEvgV+B7QRkedF5NdubDsJq7uqSob92ElEpCPQGZh/iueniMhyEVmenZ3txq6Valq6t4mi\nb1I0s1dmOh1F+Ygd2ceYvTKDq8/sSJvo058m5nS4c83sAmPMW8aY8UAysBo46XhDDWpqfZzqOpUT\ngQ+MMRWnyDDdGJNmjEmLj/fuLKBK+Yr01GTWZeax5YDOoqPgya+3EhoUyNSRXT2+r9OakMYYc9gY\n86Ix5jw3Vs8AXKfrTAZO1W6eiHY7KVWrCQMSCQwQbVUoNu8/ysdrs5g8vJNXjll58jqLy4AUEels\nX79iIjC3+koi0gNoBSz2YBal/F5ci1BGdo9nzqpMKipP1ThXzcGTX28hMiSIKed08cr+PFYojDHl\nwO3AF8DPwExjzAYReUhEJrisOgl41xijf/lK1SF9YDL784tZsiPH6SjKIesz8/h8/X5uPLszrSK9\nM3OAR2c1M8Z8BnxW7bG/VLv/oCczKNWUjOqVQFRYELNWZjC8W8Mvcan8zxNfbSEmPJgbz+nstX16\nsutJKdXIwoIDGd8/kXnr91NQUu50HOVlK/fk8s2mg0z5VZcaL6PrKVoolPIz6QOTKCyt4IsN+52O\norzsia+20DoyhMnDOnl1v1oolPIzaR1b0b51uI5+amZ+2pHDwq2HmDqiK5Ghnr0WSnVaKJTyMyJC\nemoyi7YfYl9ekdNxlBcYY3j8qy0kRIVy9Znen7xbC4VSfih9YBLGwJxVOqVHc7BoWw5Ldx7mtnO7\nER7i/anmtVAo5Yc6xkaS1rEVs1dmoCPLmzZjDI99uZnEmDAmDmlf9ws8QAuFUn4qfWAyWw8eO35N\nb9U0Ldh8kNV7j3DHqBRCg5y5cJUWCqX81AX92hESFMAsvU5Fk2WM4fEvt9ChdQSXDUp2LIcWCqX8\nVExEMKN7JTB3dRZlFZVOx1Ee8MWG/WzIyufOUSkEBzr3ca2FQik/lp6aTE5BKd9v0en3m5rKSsMT\nX22lS3wkFw9IdDSLFgql/NiIHvG0jgzRcyqaoE/W7WPzgaPcPbo7QQ62JkALhVJ+LTgwgAlnJPLV\nzwfIKypzOo5qJOUVlTz51RZGJ3kcAAATdElEQVR6tIlifL92TsfRQqGUv7t0YDKl5ZV8tm6f01FU\nI5mzOosdhwr43fndCQg4nStQe4YWCqX8XN+kaFISWjBbRz81CWUVlTz1zRb6JkXzmz5tnI4DaKFQ\nyu+JCOkDk1m2K5fdOQVOx1EN9P7yDPYeLuLe83sg4nxrArRQKNUkXJyaiAh8uEoPavuz4rIKnp6/\nldQOLRnZI97pOMdpoVCqCWgXE86wrrHMXpmpU3r4sXeX7mFfXjG//7XvtCZAC4VSTUZ6ajJ7Dhey\nYneu01FUPRSVVvDst9sZ2rk1w7rGOh3nBFoolGoixvRtS3hwILP0nAq/NGPJLrKPlnCvj7UmQAuF\nUk1GZGgQY/u25dO1WRSXVTgdR52GYyXlvPDdDs5JiWNI59ZOxzmJFgqlmpD0gcnkF5czf9NBp6Oo\n0/D6j7s4XFDKvb/u4XSUGmmhUKoJOatrLG2jw/ScCj+SV1TGi99tZ3SvBAa0b+l0nBppoVCqCQkM\nEC5KTeTbzdkcOlbidBzlhv/+sJP84nJ+d353p6OckhYKpZqY9NRkyisNH6/Ry6T6utyCUl75YSdj\n+7alT2KM03FOSQuFUk1Mj7ZR9E2K1hll/cCL3++goNS3WxOghUKpJik9NZl1mXlsPXDU6SjqFLKP\nlvD6j7uYcEYi3dtEOR2nVloolGqCJgxIJDBAmK1Tevis57/dTkl5BXeNSnE6Sp20UCjVBMW1CGVk\n93jmrMqkolKn9PA1+/OKefOn3Vw6MJku8S2cjlMnLRRKNVHpA5PZl1fMkh05TkdR1Ty7YBuVlYY7\n/aA1AVoolGqyRvVKICosiFl6ToVPycgt5N1le/jt4Pa0bx3hdBy3aKFQqokKCw5kfP92zFu/n4KS\ncqfjKNvT32xDRLj9vG5OR3GbFgqlmrD0gckUllbwxYb9TkdRwK5DBXywMoMrh3SgXUy403HcpoVC\nqSYsrWMr2rcO13MqfMRT32wlOFCYdm5Xp6OcFi0USjVhIkJ6ajKLth9iX16R03Gata0HjjJndSbX\nndWJhKgwp+OcFi0USjVx6QOTMAY+Wq1Tejjpya+3EhEcyC0j/Ks1AR4uFCIyRkQ2i8g2Ebn/FOtc\nISIbRWSDiLztyTxKNUcdYyNJ69iKWSsy9DKpDtmYlc+n6/Zxw9mdaR0Z4nSc0+axQiEigcCzwFig\nNzBJRHpXWycF+CMw3BjTB7jbU3mUas7SByaz9eAxNmTlOx2lWfr3V1uICgviprO7OB2lXjzZohgC\nbDPG7DDGlALvAhdVW+dm4FljTC6AMUavtqKUB1zQrx0hgQF6ToUD1uw9wtc/H2DKOV2IiQh2Ok69\neLJQJAF7Xe5n2I+56g50F5FFIrJERMbUtCERmSIiy0VkeXZ2tofiKtV0xUQEM7p3AnNXZ1FWUel0\nnGbl319toVVEMNef3dnpKPXmyUJR09XBq3eQBgEpwEhgEvCyiJx0iSdjzHRjTJoxJi0+Pr7RgyrV\nHKSnJpNTUMr3W/TLlrcs33WY77Zkc+uIrrQIDXI6Tr15slBkAO1d7icD1YddZAAfGWPKjDE7gc1Y\nhUMp1chG9IindWSInlPhRY9/uYW4FqFce1Ynp6M0iCcLxTIgRUQ6i0gIMBGYW22dOcC5ACISh9UV\ntcODmZRqtoIDA5hwRiJf/XyAvKIyp+M0eT9uO8TiHTlMG9mV8JBAp+M0iMcKhTGmHLgd+AL4GZhp\njNkgIg+JyAR7tS+AHBHZCCwA/mCM0akulfKQSwcmU1peyWfr9jkdpUkzxvD4V1toGx3GlUM7OB2n\nwTzaaWaM+Qz4rNpjf3FZNsA99k0p5WF9k6JJSWjB7JUZTBri/x9gvuq7Ldms2J3Lwxf3JSzYv1sT\noGdmK9WsiAiXDExi2a5cducUOB2nSTLG8O+vtpDcKpwr0trX/QI/oIVCqWbm4gFJiMCHeplUj/hq\n4wHWZuRx56gUQoKaxkes/47XUkrVS2LLcIZ1jWX2ykzuGpWCSE0j2VVdjDHkFZVxIL+Eg0eLOZhf\nwsGjJcxcvpfOcZGkp1Y/bcx/aaFQqhlKT03m3vfXsGJ3LmmdWjsdx6dUVBpyCko4mF9C9tETi8DB\no8XWv/klZB8robT85JMXo8OCeGpiKkGBTaM1AVoolGqWxvRtywNz1jN7VWazKRSl5ZVkHyvhYL79\nYX+0hGyX5aqCkFNQSkXlyZMntowIJiEqlISoMIZ2jiQ+2lq2HgslIdpajvTjE+tOpen9REqpOkWG\nBjG2b1s+WZPFX8b39uuROYWl5Sd+43dZzra//R88Wkxu4cnnjohAbGTVB30ovdtFWx/+0dZj8XYh\niI8K9ev3qKG0UCjVTKUPTGb2qkzmbzrIuH7tnI5z2t5fvpdHPt/E4YLSk54LDhTiW4QSHx1Gh9gI\n0jq1OqEAVC3HRoY0qS4iT9FCoVQzdVbXWNpEhzJ7ZYZfFYqS8goenLuRd5buYUin1ow4O/54108b\nuzuoZXgwAQF6kL6xaKFQqpkKDBAuTk3ivwt3cuhYCXEtQp2OVKfMI0VMe3MFazLymDqyK/ee311b\nBF6g77BSzVh6ajLllYaP1/j+ZVIXbs1m/H8WsiO7gBevGcT/jOmpRcJL9F1Wqhnr0TaKvknRPj2j\nbGWl4Zn5W7n2laUkRIUx946z+U2ftk7Hala0UCjVzKWnJrMuM4+tB446HeUkeUVlTJmxnMe+3MKE\nMxL58LZhdI6LdDpWs6OFQqlmbsKARAIDhNk+NqXHxqx8JjzzA99uzuZvE/rw5G8HEBGih1WdoIVC\nqWYurkUoI7vHM2dVZo0nmjlh1ooM0p9fRHFZBe/dcibXDeukU404SAuFUopLBiaxL6+YJTucvRxM\nSXkFD8xZx73vr2FA+5Z8csc5DOrYPM4c92XajlNKMbpXG6LCgpi1MoPh3eIcyZB1pIhpb61k9d4j\n3PKrLvzhNz10VJOP0N+CUoqw4EDG92/HvPX7KSgp9/r+F207xPinf2DbwWM8f9VA/jiulxYJH6K/\nCaUUYE3pUVhawRcb9nttn8YYnvt2G9f89ydiI0P46PbhjPWjs8SbCy0USikA0jq2on3rcK9d0Ci/\nuIwpM1bw6LzNjOvXjjm3DadrfAuv7FudHj1GoZQCrMukpqcm85/5W9mfV0zbmDCP7WvT/nxunbGC\njNwi/jK+N9cP11FNvkxbFEqp49IHJmEMzFntuVbFnFWZXPLsjxSUVvDOlDO54ezOWiR8nBYKpdRx\nHWMjGdSxFbNWZGBM455TUVpeyV8/Ws/d762mX1IMn95xNoObyUWT/J0WCqXUCdIHJrH14DE2ZOU3\n2jb35xUzcfpiXl+8m5vO7sxbNw8lIdpzXVuqcWmhUEqdYHy/REICA5i1MqNRtvfj9kOMf3ohm/Yf\n5dkrB/LA+N4E69BXv6K/LaXUCWIighndO4G5q7Moq6is93aMMbz43XaufvknYsKDmXv7cC7or0Nf\n/ZEWCqXUSdJTk8kpKGXh1ux6vf5ocRlT31zJI59vYkzftnx0+9l0S4hq5JTKW3R4rFLqJCN6xNM6\nMoRZKzM5r2eb03rtlgNHuXXGCnYfLuSBC3pxo45q8ntaKJRSJwkODGDCGYm8vXQPeUVlxIQHu/W6\nj9dk8T+z1hIREsTbNw1laJdYDydV3qBdT0qpGqUPTKK0vJLP1u2rc92yikr+9vEG7nhnFb3bRfPp\nnWdrkWhCtFAopWrULymGbgktmF3H6KeD+cVMmr6EVxft4vrhnXhnypm00aGvTYoWCqVUjUSE9IFJ\nLNuVy+6cghrX+WlHDuP+8wMbsvL5z6RU/nphHx362gTpb1QpdUoXD0hChJMmCjTG8PLCHVz58k9E\nhwXx0e3DmXBGokMpladpoVBKnVJiy3CGdY3lw1WZx6f0OFZSzu1vr+LhT3/m/F5t+Oj24XRvo0Nf\nmzId9aSUqlV6ajL3vr+GlXtyiQkP5pYZK9h5qIA/ju3JlF910aGvzYAWCqVUrcb0bcsDc9bz8Kc/\ns2X/UcJDAnnzpqEM6+rMJVOV92nXk1KqVpGhQYzt25ZVe47Qo20Un9xxjhaJZkYaeyphTxORo8Bm\np3NUEwcccjpENb6YCXwzl2Zyj2Zyny/m6mGMqdfBJH/setpsjElzOoQrEVmumdzji7k0k3s0k/t8\nMZeILK/va7XrSSmlVK20UCillKqVPxaK6U4HqIFmcp8v5tJM7tFM7vPFXPXO5HcHs5VSSnmXP7Yo\nlFJKeZEWCqWUUrXy2UIhImNEZLOIbBOR+2t4PlRE3rOf/0lEOvlApl+JyEoRKReRyzydx81M94jI\nRhFZKyLfiEhHH8h0q4isE5HVIvKDiPT2dCZ3crmsd5mIGBHx+PBGN96rySKSbb9Xq0XkJqcz2etc\nYf9dbRCRt53OJCJPuLxHW0TkiA9k6iAiC0Rklf3/b5ynM7mZq6P9WbBWRL4VkeQ6N2qM8bkbEAhs\nB7oAIcAaoHe1daYBL9jLE4H3fCBTJ6A/8AZwmY+8T+cCEfbyVB95n6JdlicA83zhvbLXiwK+B5YA\naU5nAiYDz3j6/TnNTCnAKqCVfT/B6UzV1r8DeMXpTFgHj6fay72BXT7y+3sfuM5ePg+YUdd2fbVF\nMQTYZozZYYwpBd4FLqq2zkXA6/byB8Ao8ezsZHVmMsbsMsasBSo9mON0My0wxhTad5cAdX978Hym\nfJe7kYA3RlS48zcF8H/Ao0CxD2XyJncy3Qw8a4zJBTDGHPSBTK4mAe/4QCYDRNvLMUCWhzO5m6s3\n8I29vKCG50/iq4UiCdjrcj/DfqzGdYwx5UAe4MlrL7qTydtON9ONwOceTeRmJhG5TUS2Y30o3+nh\nTG7lEpFUoL0x5hMv5HErk+1Su5vgAxFp7wOZugPdRWSRiCwRkTE+kAmwulWAzsB8H8j0IHC1iGQA\nn2G1dDzNnVxrgEvt5UuAKBGp9bPTVwtFTS2D6t863VmnMXl7f+5wO5OIXA2kAf/yaCI3MxljnjXG\ndAX+B3jAw5mgjlwiEgA8AdzrhSzHd1vDY9Xfq4+BTsaY/sDX/NKKdjJTEFb300isb+8vi0hLhzNV\nmQh8YIyp8GAecC/TJOA1Y0wyMA6YYf+dOZ3r98AIEVkFjAAygfLaNuqrhSIDcP3mlMzJzbbj64hI\nEFbT7rDDmbzNrUwiMhr4MzDBGFPiC5lcvAtc7NFElrpyRQF9gW9FZBdwJjDXwwe063yvjDE5Lr+z\nl4BBHszjViZ7nY+MMWXGmJ1Yk3SmOJypykQ83+0E7mW6EZgJYIxZDIRhTRboaC5jTJYxJt0Yk4r1\nuYAxJq/WrXr64Eo9D8gEATuwmpBVB2T6VFvnNk48mD3T6Uwu676Gdw5mu/M+pWId3Erxod9disvy\nhcByX8hVbf1v8fzBbHfeq3Yuy5cAS3wg0xjgdXs5DqurI9bp3x3QA9iFfSKxD7xPnwOT7eVeWB/Y\nHs3mZq44IMBe/n/AQ3Vu19NvaAN+4HHAFvtD7s/2Yw9hfSsGqzq/D2wDlgJdfCDTYKyKXgDkABt8\nINPXwAFgtX2b6wOZngI22HkW1PaB7c1c1db9Fg8XCjffq0fs92qN/V719IFMAvwb2AisAyY6ncm+\n/yDwD2/8Lbn5PvUGFtm/u9XAr30k12XAVnudl4HQurapU3gopZSqla8eo1BKKeUjtFAopZSqlRYK\npZRStdJCoZRSqlZaKJRSStVKC4XyeyJyzAPb3CUidZ4c5Yl91yeHUp6khUIppVSttFCoJklELrSv\nU7JKRL4WkTb24w+KyOsi8qX9bT1dRB61r48xT0SCXTbzBxFZat+62a/vLCKLRWSZiPyfy/5a2HP8\nr7S3ddKMnCIyVUQedbk/WUSetpfniMgK+/oOU2p4bScRWe9y//ci8qC93NXOvkJEFopIz4a/g0r9\nQguFaqp+AM401nw27wL3uTzXFbgAa3rlN4EFxph+QJH9eJV8Y8wQ4BngSfuxp4DnjTGDgf0u6xYD\nlxhjBmJdA+TxGqa9/wBId7n/W+A9e/kGY8wgrIkb76xrNs9qpgN32K//PfDcabxWqToFOR1AKQ9J\nBt4TkXZYc97sdHnuc2NMmYisw7rQyzz78XVYF5+q8o7Lv0/Yy8P5ZYrmGcA/7WUB/i4iv8K6HkkS\n0AaXYmKMyRaRHSJyJtYUCj2wpngAqzhcYi+3x5pkL6euH1JEWgDDgPdd6lJoXa9T6nRooVBN1dPA\nv40xc0VkJNY8QFVKAIwxlSJSZn6Zx6aSE/9PGDeWq1wFxAOD7CK0C2s+sureA64ANgEfGmOMnW80\ncJYxplBEvq3hteWc2ANQ9XwAcMQYM6CGfSnVKLTrSTVVMVjz7ANcV89t/Nbl38X28iKs2YrBKg6u\n+ztoF4lzgVNdm3w21rTqk/il2ykGyLWLRE+sKc6rOwAkiEisiIQC4+H41QJ3isjlAGI54zR/TqVq\npYVCNQURIpLhcrsHqwXxvogsBA7Vc7uhIvITcBfwO/uxu4DbRGQZ1gd8lbeANBFZjlVANtW0QWNd\nPnQj0NEYs9R+eB4QJCJrsS7FuqSG15VhzQD6E/BJte1fBdwoImuwZpp1+nKqqonR2WOVUkrVSlsU\nSimlaqWFQimlVK20UCillKqVFgqllFK10kKhlFKqVloolFJK1UoLhVJKqVr9f4/gigTIzAMTAAAA\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aec93fd6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best lambda value 0.0005\n"
     ]
    }
   ],
   "source": [
    "lams = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "accuracy =[]\n",
    "for lam in lams:\n",
    "    lr = LogReg(train_set, test_set, lam, eta = 0.1)\n",
    "    lr.train()\n",
    "    NLL, acc = lr.compute_progress(test_set)\n",
    "    accuracy.append(round(acc, 4))\n",
    "\n",
    "print(accuracy)\n",
    "plt.xlim(0, lams[-1])\n",
    "plt.ylim(min(accuracy),1.0)\n",
    "plt.xlabel(\"Lambda value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Lambda vs Accuracy\")\n",
    "plt.plot(lams, accuracy)\n",
    "plt.show()\n",
    "\n",
    "bestlam = lams[accuracy.index(max(accuracy))]\n",
    "print(\"best lambda value\",bestlam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: For the value of `lam` chosen in **Part A** perform a systematic study of the choice of learning rate on the speed of convergence SGD.  Which learning rate seems to give the fastest convergence?  Justify your conclusion with some kind of graphic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged for 742.349891737 742.346644868 742.344353489\n",
      "Converged for 742.332513119 742.321317324 742.34085585\n",
      "Converged for 742.239599249 742.239163717 742.217602075\n",
      "Converged for 742.155968713 741.625848109 741.594401351\n",
      "Converged for 741.621440619 741.61725239 741.583891511\n",
      "Converged for 730.476199943 729.797655696 729.460154231\n",
      "Converged for 648.638495909 647.932891611 647.698667469\n",
      "Converged for 591.329691891 591.261106421 591.702357999\n",
      "Converged for 569.544590585 569.926386398 569.936547837\n",
      "Converged for 761.626339544 760.877291595 760.394862749\n",
      "Converged for 698.59913552 698.865981502 698.27309221\n",
      "eta_value vs eta_iterations {1e-05: 4, 5e-05: 4, 0.0001: 4, 0.0005: 4, 0.001: 4, 0.005: 9, 0.01: 54, 0.05: 170, 0.1: 205, 0.3: 2085, 0.4: 588}\n"
     ]
    }
   ],
   "source": [
    "eta_vals = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "conv_threshold = 0.8\n",
    "iterations =0\n",
    "prevnll  = 0\n",
    "prevprevnll =0\n",
    "train_nll = 0\n",
    "eta_iterations = {}\n",
    "\n",
    "\n",
    "def Convergence(prevprevnll, prevnll, train_nll, conv_threshold):\n",
    "    if(abs(prevnll - prevprevnll) < conv_threshold and abs(prevnll - train_nll) < conv_threshold ):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "for eta in eta_vals:\n",
    "    lr = LogReg(train_set, test_set, lam=0.5, eta = eta)\n",
    "    i=0\n",
    "    np.random.shuffle(lr.train_set)\n",
    "    # loop over each training example\n",
    "\n",
    "    for ex in lr.train_set:\n",
    "        # perform SGD update of weights \n",
    "        lr.sgd_update(ex, iterations)\n",
    "        prevprevnll = prevnll \n",
    "        prevnll = train_nll\n",
    "        train_nll, train_acc = lr.compute_progress(lr.train_set)\n",
    "\n",
    "        if(iterations > 3):\n",
    "            if Convergence(prevprevnll, prevnll, train_nll, conv_threshold):\n",
    "                i=11 #come out of loop\n",
    "                eta_iterations[eta] = iterations\n",
    "                iterations =0\n",
    "                break\n",
    "                    \n",
    "        iterations += 1\n",
    "print(\"eta_value vs eta_iterations\", eta_iterations)\n",
    "\n",
    "\n",
    "##################### Eta_val vs Accuracy################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the convergence function is kind of approximate. If we optimize it and with a accurate threshold, the above code would show the difference of iterations with respect to changing eta values appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9091, 0.9174, 0.9174, 0.9504, 0.9256, 0.9421, 0.9421, 0.9339, 0.9256, 0.9339, 0.9174, 0.9256, 0.8926, 0.8264, 0.9174, 0.9174, 0.8926]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEWCAYAAABIVsEJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aec710e2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best eta value 0.0005\n"
     ]
    }
   ],
   "source": [
    "eta_vals = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "accuracy =[]\n",
    "for eta in eta_vals:\n",
    "    lr = LogReg(train_set, test_set, bestlam, eta)\n",
    "    lr.train()\n",
    "    NLL, acc = lr.compute_progress(test_set)\n",
    "    accuracy.append(round(acc, 4))\n",
    "\n",
    "print(accuracy)\n",
    "plt.xlim(0, eta_vals[-1])\n",
    "plt.ylim(min(accuracy),1.0)\n",
    "plt.xlabel(\"eta value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"eta vs Accuracy\")\n",
    "plt.plot(eta_vals, accuracy)\n",
    "plt.show()\n",
    "\n",
    "besteta = eta_vals[accuracy.index(max(accuracy))]\n",
    "print(\"best eta value\",besteta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 4: Identifying Predictive and Non-Predictive Words \n",
    "***\n",
    "\n",
    "**Part A**: Find the top 10 words that are the best predictors for each class.  Explain mathematically how you identified them and show any code that you used to find them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I think, the words with higher value of weights would have higher impact to class 1. Hence, those would be the best predictors due to higher impact. In order to get the top 10 words, we have to find the words with top 10 larger weights. Similarly the 10 best predictors for class 0 would be the words with lowest 10 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best predictors for motorcycle ['bike', 'like', 'deductible', 'bikes', 'ca', 'ticket', 'berkeley', 'tire', 'mil', 'old']\n",
      "Best predictors for automobiless ['side', 'clothing', 'cars', 'left', 'reply', 'hp', 'day', 'sweden', 'dallas', 'les']\n"
     ]
    }
   ],
   "source": [
    "lr = LogReg(train_set, test_set, 0.01, 0.5)\n",
    "lr.train()\n",
    "best_positive_class_predictors =[]\n",
    "best_negative_class_predictors = []\n",
    "weights = list(lr.w)\n",
    "descendingweights = sorted(weights, reverse=True)\n",
    "ascendingweights = sorted(weights)\n",
    "toptenWeights = descendingweights[:10]\n",
    "lowesttenweights = ascendingweights[:10]\n",
    "\n",
    "for weight in toptenWeights:\n",
    "    best_positive_class_predictors.append(vocab[weights.index(weight)])\n",
    "    \n",
    "for weight in lowesttenweights:\n",
    "    best_negative_class_predictors.append(vocab[weights.index(weight)])\n",
    "    \n",
    "print(\"Best predictors for motorcycle\", best_positive_class_predictors)\n",
    "print(\"Best predictors for automobiless\", best_negative_class_predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Find the 10 words that are the worst predictors for class.  Explain mathematically how you identified them and show any code that you used to find them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Same concept applies here, the worst predictors for class have values which are in the middle of the sorted weights, As they can be predicted as class 0 or 1. Hence, once we sort the weighted values, the center values are the ones which are worst predictor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst predictors for motorcycle ['tolerable', 'hired', 'becomes', 'tuesday', 'camber', 'sizes', 'corvettes', 'driveways', 'expenses', 'towing']\n",
      "Worst predictors for automobiless ['probes', 'reads', 'swap', 'winters', 'qualities', 'grip', 'aiming', 'underside', 'eagles', 'goodyear']\n"
     ]
    }
   ],
   "source": [
    "mid = int(len(ascendingweights)/2)\n",
    "worst_positive_class_predictors = []\n",
    "worst_negative_class_predictors = []\n",
    "\n",
    "worstnegativeweights = ascendingweights[mid - 10:mid]\n",
    "worstpositiveweights = ascendingweights[mid :mid + 10]\n",
    "\n",
    "for weight in worstpositiveweights:\n",
    "    worst_positive_class_predictors.append(vocab[weights.index(weight)])\n",
    "    \n",
    "for weight in worstnegativeweights:\n",
    "    worst_negative_class_predictors.append(vocab[weights.index(weight)])\n",
    "    \n",
    "print(\"Worst predictors for motorcycle\", worst_positive_class_predictors)\n",
    "print(\"Worst predictors for automobiless\", worst_negative_class_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
